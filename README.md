# 🧠 Text Processing and Tokenization

This project provides a modular pipeline for processing and tokenizing text datasets using Python's NLP ecosystem — including `spaCy`, `pandas`, and `nltk`. It is designed for practical tasks like lemmatization or stemming, with compatibility for large datasets such as [Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140).

---

## 💡 Key Features

- Load raw CSV datasets with custom column mappings
- Clean and normalize text (e.g., remove noise, lowercase, etc.)
- Tokenize efficiently using `spaCy` pipelines
- Filter out stop words, punctuation, and non-alphabetic tokens
- Automatically ensure required language models are installed

## 🔍 Notes
Designed to work with large-scale CSV datasets (e.g., Kaggle's Sentiment140)

Compatible with virtual environments and reproducible installs

Clean .gitignore prevents committing data, logs, or virtual environments

## 📄 License
MIT License – free for personal and commercial use.

