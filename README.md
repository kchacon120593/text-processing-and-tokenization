# ğŸ§  Text Processing and Tokenization

This project provides a modular pipeline for processing and tokenizing text datasets using Python's NLP ecosystem â€” including `spaCy`, `pandas`, and `nltk`. It is designed for practical tasks like lemmatization or stemming, with compatibility for large datasets such as [Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140).

---

## ğŸ’¡ Key Features

- Load raw CSV datasets with custom column mappings
- Clean and normalize text (e.g., remove noise, lowercase, etc.)
- Tokenize efficiently using `spaCy` pipelines
- Filter out stop words, punctuation, and non-alphabetic tokens
- Automatically ensure required language models are installed

## ğŸ” Notes
Designed to work with large-scale CSV datasets (e.g., Kaggle's Sentiment140)

Compatible with virtual environments and reproducible installs

Clean .gitignore prevents committing data, logs, or virtual environments

## ğŸ“„ License
MIT License â€“ free for personal and commercial use.

